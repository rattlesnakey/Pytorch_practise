{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 时间维度上的线性加和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASP(torch.nn.Module):\n",
    "    ''' Attentive Statistics Pooling 注意力统计池化\n",
    "\n",
    "        Okabe K , Koshinaka T , Shinoda K . Attentive Statistics Pooling for Deep Speaker Embedding[J]. 2018.\n",
    "    '''\n",
    "    def __init__(self, channel):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=channel, out_channels=channel, kernel_size=1)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=channel, out_channels=1, kernel_size=1)\n",
    "\n",
    "        self.conv1 = torch.nn.utils.weight_norm(self.conv1)\n",
    "        self.conv2 = torch.nn.utils.weight_norm(self.conv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x:\",x.shape)\n",
    "        o = self.conv1(x).relu()     # [B, C, T]\n",
    "        print(\"conv1:\",o.shape)\n",
    "        #conv2感觉就是里面已经包括了乘上T了\n",
    "        o = self.conv2(o).softmax(2) # [B, 1, T],在时间维度上softmax，也就是时间维度上加和为0\n",
    "        print(\"conv2:\",o.shape)\n",
    "\n",
    "        u = (o*x).sum(2)          # [B, C]，o首先会复制扩展为[B,C,T],C行里面的每一行都和[1,2...T]是一样的\n",
    "                                #也就是说o*x是先时间维度上都先乘上一个权值，然后对每一行求和\n",
    "        print(\"u:\",u.shape)\n",
    "        s = (o*x**2).sum(2)-u**2  # [B, C]\n",
    "        print(\"s:\",s.shape)\n",
    "\n",
    "        return torch.cat([u,s**0.5],1)     # [B, 2C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASP(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(42,39,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([42, 39, 135])\n",
      "conv1: torch.Size([42, 39, 135])\n",
      "conv2: torch.Size([42, 1, 135])\n",
      "u: torch.Size([42, 39])\n",
      "s: torch.Size([42, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5338, 0.4669, 0.5200,  ..., 0.3101, 0.2956, 0.2848],\n",
       "        [0.4621, 0.4887, 0.5111,  ..., 0.2922, 0.2771, 0.2813],\n",
       "        [0.5171, 0.4744, 0.4782,  ..., 0.2864, 0.2823, 0.2931],\n",
       "        ...,\n",
       "        [0.4885, 0.4930, 0.5019,  ..., 0.2850, 0.2883, 0.3005],\n",
       "        [0.4788, 0.4366, 0.4422,  ..., 0.2658, 0.2730, 0.2883],\n",
       "        [0.4829, 0.5137, 0.5379,  ..., 0.2701, 0.2771, 0.2840]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 135])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.rand(42,1,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 135])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = x * weight\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0900, 0.2447, 0.6652],\n",
       "         [0.0900, 0.2447, 0.6652]],\n",
       "\n",
       "        [[0.0900, 0.2447, 0.6652],\n",
       "         [0.0900, 0.2447, 0.6652]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.float().softmax(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6, 15],\n",
       "        [24, 33]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.sum(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weight = torch.tensor([[[1,2,3]],[[4,5,6]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3]],\n",
       "\n",
       "        [[4, 5, 6]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  4,  9],\n",
       "         [ 4, 10, 18]],\n",
       "\n",
       "        [[28, 40, 54],\n",
       "         [40, 55, 72]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1*test_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weight2 = torch.tensor([[[1],[2]],[[3],[4]]])\n",
    "test_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 8, 10, 12]],\n",
       "\n",
       "        [[21, 24, 27],\n",
       "         [40, 44, 48]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 * test_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myASP(torch.nn.Module):\n",
    "    ''' Attentive Statistics Pooling 注意力统计池化\n",
    "\n",
    "        Okabe K , Koshinaka T , Shinoda K . Attentive Statistics Pooling for Deep Speaker Embedding[J]. 2018.\n",
    "    '''\n",
    "    def __init__(self, channel):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=channel, out_channels=channel, kernel_size=1)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=channel, out_channels= 1 , kernel_size=1)\n",
    "#         self.v = nn.Parameter(torch.rand(,))\n",
    "        self.conv1 = torch.nn.utils.weight_norm(self.conv1)\n",
    "        self.conv2 = torch.nn.utils.weight_norm(self.conv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x:\",x.shape)\n",
    "        o = self.conv1(x).relu()     # [B, C, T]\n",
    "        print(\"conv1:\",o.shape)\n",
    "        #好像是conv1然后再conv2才得到权重，conv2的作用好像就是v,因为其实v的话是扫描每个时间维度，然后对通道线性加和得到一个值\n",
    "        o = self.conv2(o).softmax(2) # [B, 1, T],在时间维度上softmax，也就是时间维度上加和为0\n",
    "        print(\"conv2:\",o.shape)\n",
    "        #这里的x应该改成用e_t\n",
    "        u = (o*x).sum(2)          # [B, C]，o首先会复制扩展为[B,C,T],C行里面的每一行都和[1,2...T]是一样的\n",
    "                                #也就是说o*x是先时间维度上都先乘上一个权值，然后对每一行求和\n",
    "        print(\"u:\",u.shape)\n",
    "        s = (o*x**2).sum(2)-u**2  # [B, C]\n",
    "        print(\"s:\",s.shape)\n",
    "\n",
    "        return torch.cat([u,s**0.5],1)     # [B, 2C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myASP2(torch.nn.Module):\n",
    "    ''' Attentive Statistics Pooling 注意力统计池化\n",
    "\n",
    "        Okabe K , Koshinaka T , Shinoda K . Attentive Statistics Pooling for Deep Speaker Embedding[J]. 2018.\n",
    "    '''\n",
    "    def __init__(self, channel):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=channel, out_channels=channel, kernel_size=1)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=channel, out_channels= channel , kernel_size=1)\n",
    "#         self.v = nn.Parameter(torch.rand(,))\n",
    "        self.conv1 = torch.nn.utils.weight_norm(self.conv1)\n",
    "        self.conv2 = torch.nn.utils.weight_norm(self.conv2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x:\",x.shape)\n",
    "        o = self.conv1(x).relu()     # [B, C, T]\n",
    "        print(\"conv1:\",o.shape)\n",
    "        #好像是conv1然后再conv2才得到权重，conv2的作用好像就是v,因为其实v的话是扫描每个时间维度，然后对通道线性加和得到一个值\n",
    "        o = self.conv2(o).softmax(2) # [B, C, T],在时间维度上softmax，也就是时间维度上加和为0\n",
    "        print(\"conv2:\",o.shape)\n",
    "        #这里的x应该改成用e_t\n",
    "        u = (o*x).sum(2)          # [B, C]，o首先会复制扩展为[B,C,T],C行里面的每一行都和[1,2...T]是一样的\n",
    "                                #也就是说o*x是先时间维度上都先乘上一个权值，然后对每一行求和\n",
    "        print(\"u:\",u.shape)\n",
    "        s = (o*x**2).sum(2)-u**2  # [B, C]\n",
    "        print(\"s:\",s.shape)\n",
    "\n",
    "        return torch.cat([u,s**0.5],1)     # [B, 2C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = myASP2(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([42, 39, 135])\n",
      "conv1: torch.Size([42, 39, 135])\n",
      "conv2: torch.Size([42, 39, 135])\n",
      "u: torch.Size([42, 39])\n",
      "s: torch.Size([42, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5270, 0.4705, 0.5324,  ..., 0.3098, 0.2953, 0.2856],\n",
       "        [0.4577, 0.4902, 0.5157,  ..., 0.2905, 0.2775, 0.2822],\n",
       "        [0.5090, 0.4753, 0.4802,  ..., 0.2862, 0.2827, 0.2914],\n",
       "        ...,\n",
       "        [0.4829, 0.4920, 0.5042,  ..., 0.2845, 0.2883, 0.3016],\n",
       "        [0.4726, 0.4381, 0.4485,  ..., 0.2669, 0.2723, 0.2894],\n",
       "        [0.4757, 0.5119, 0.5442,  ..., 0.2702, 0.2745, 0.2847]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "### There are some basic custom components/layers. ###\n",
    "\n",
    "## Base ✿\n",
    "class TdnnAffine(torch.nn.Module):\n",
    "    \"\"\" An implemented tdnn affine component by conv1d\n",
    "        y = splice(w * x, context) + b\n",
    "    @input_dim: number of dims of frame <=> inputs channels of conv\n",
    "    @output_dim: number of layer nodes <=> outputs channels of conv\n",
    "    @context: a list of context\n",
    "        e.g.  [-2,0,2]\n",
    "    If context is [0], then the TdnnAffine is equal to linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, context=[0], bias=True, pad=True, stride=1, groups=1, norm_w=False, norm_f=False):\n",
    "        super(TdnnAffine, self).__init__()\n",
    "        assert input_dim % groups == 0\n",
    "        # Check to make sure the context sorted and has no duplicated values\n",
    "        for index in range(0, len(context) - 1):\n",
    "            if(context[index] >= context[index + 1]):\n",
    "                raise ValueError(\"Context tuple {} is invalid, such as the order.\".format(context))\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.context = context\n",
    "        self.bool_bias = bias\n",
    "        self.pad = pad\n",
    "        self.groups = groups\n",
    "\n",
    "        self.norm_w = norm_w\n",
    "        self.norm_f = norm_f\n",
    "\n",
    "        # It is used to subsample frames with this factor\n",
    "        self.stride = stride\n",
    "\n",
    "        self.left_context = context[0] if context[0] < 0 else 0 \n",
    "        self.right_context = context[-1] if context[-1] > 0 else 0 \n",
    "\n",
    "        self.tot_context = self.right_context - self.left_context + 1\n",
    "\n",
    "        # Do not support sphereConv now.\n",
    "        if self.tot_context > 1 and self.norm_f:\n",
    "            self.norm_f = False\n",
    "            print(\"Warning: do not support sphereConv now and set norm_f=False.\")\n",
    "\n",
    "        kernel_size = (self.tot_context,)\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_dim, input_dim//groups, *kernel_size))\n",
    "\n",
    "        if self.bool_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.randn(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # init weight and bias. It is important\n",
    "        self.init_weight()\n",
    "\n",
    "        # Save GPU memory for no skiping case\n",
    "        if len(context) != self.tot_context:\n",
    "            # Used to skip some frames index according to context\n",
    "            self.mask = torch.tensor([[[ 1 if index in context else 0 \\\n",
    "                                        for index in range(self.left_context, self.right_context + 1) ]]])\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "        ## Deprecated: the broadcast method could be used to save GPU memory, \n",
    "        # self.mask = torch.randn(output_dim, input_dim, 0)\n",
    "        # for index in range(self.left_context, self.right_context + 1):\n",
    "        #     if index in context:\n",
    "        #         fixed_value = torch.ones(output_dim, input_dim, 1)\n",
    "        #     else:\n",
    "        #         fixed_value = torch.zeros(output_dim, input_dim, 1)\n",
    "\n",
    "        #     self.mask=torch.cat((self.mask, fixed_value), dim = 2)\n",
    "\n",
    "        # Save GPU memory of thi case.\n",
    "\n",
    "        self.selected_device = False\n",
    "\n",
    "    def init_weight(self):\n",
    "        # Note, var should be small to avoid slow-shrinking\n",
    "        torch.nn.init.normal_(self.weight, 0., 0.01)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.constant_(self.bias, 0.)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        @inputs: a 3-dimensional tensor (a batch), including [samples-index, frames-dim-index, frames-index]\n",
    "        \"\"\"\n",
    "        assert len(inputs.shape) == 3\n",
    "        assert inputs.shape[1] == self.input_dim\n",
    "\n",
    "        # Do not use conv1d.padding for self.left_context + self.right_context != 0 case.\n",
    "        if self.pad:\n",
    "            inputs = F.pad(inputs, (-self.left_context, self.right_context), mode=\"constant\", value=0)\n",
    "\n",
    "        assert inputs.shape[2] >=  self.tot_context\n",
    "\n",
    "        if not self.selected_device and self.mask is not None:\n",
    "            # To save the CPU -> GPU moving time\n",
    "            # Another simple case, for a temporary tensor, jus specify the device when creating it.\n",
    "            # such as, this_tensor = torch.tensor([1.0], device=inputs.device)\n",
    "            self.mask = to_device(self, self.mask)\n",
    "            self.selected_device = True\n",
    "\n",
    "        filters = self.weight  * self.mask if self.mask is not None else self.weight\n",
    "\n",
    "        if self.norm_w:\n",
    "            filters = F.normalize(filters, dim=1)\n",
    "\n",
    "        if self.norm_f:\n",
    "            inputs = F.normalize(inputs, dim=1)\n",
    "\n",
    "        outputs = F.conv1d(inputs, filters, self.bias, stride=self.stride, padding=0, dilation=1, groups=self.groups)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '{input_dim}, {output_dim}, context={context}, bias={bool_bias}, stride={stride}, ' \\\n",
    "               'pad={pad}, groups={groups}, norm_w={norm_w}, norm_f={norm_f}'.format(**self.__dict__)\n",
    "\n",
    "    @classmethod\n",
    "    def thop_count(self, m, x, y):\n",
    "        x = x[0]\n",
    "\n",
    "        kernel_ops = torch.zeros(m.weight.size()[2:]).numel()  # Kw x Kh\n",
    "        bias_ops = 1 if m.bias is not None else 0\n",
    "\n",
    "        # N x Cout x H x W x  (Cin x Kw x Kh + bias)\n",
    "        total_ops = y.nelement() * (m.input_dim * kernel_ops + bias_ops)\n",
    "\n",
    "        m.total_ops += torch.DoubleTensor([int(total_ops)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention-based\n",
    "class AttentionAlphaComponent(torch.nn.Module):\n",
    "    \"\"\"Compute the alpha with attention module.\n",
    "            alpha = softmax(v'·f(w·x + b) + k) or softmax(v'·x + k)\n",
    "    where f is relu here and bias could be lost.\n",
    "    Support: \n",
    "            1. Single or Multi-head attention\n",
    "            2. One affine or two affine\n",
    "            3. Share weight (last affine = vector) or un-shared weight (last affine = matrix)\n",
    "            4. Self-attention or time context attention (supported by context parameter of TdnnAffine)\n",
    "            5. Different temperatures for different heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_head=1, split_input=True, share=True, affine_layers=2, \n",
    "                 hidden_size=64, context=[0], bias=True, temperature=False, fixed=True):\n",
    "        super(AttentionAlphaComponent, self).__init__()\n",
    "        assert num_head >= 1\n",
    "        # Multi-head case.\n",
    "        if num_head > 1:\n",
    "            if split_input:\n",
    "                # Make sure fatures/planes with input_dim dims could be splited to num_head parts.\n",
    "                assert input_dim % num_head == 0\n",
    "            if temperature:\n",
    "                if fixed:\n",
    "                    t_list = []\n",
    "                    for i in range(num_head):\n",
    "                        t_list.append([[max(1, (i // 2) * 5)]])\n",
    "                    # shape [1, num_head, 1, 1]\n",
    "                    self.register_buffer('t', torch.tensor([t_list]))\n",
    "                else:\n",
    "                    # Different heads have different temperature.\n",
    "                    # Use 1 + self.t**2 in forward to make sure temperature >= 1.\n",
    "                    self.t = torch.nn.Parameter(torch.zeros(1, num_head, 1, 1))\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_head = num_head\n",
    "        self.split_input = split_input\n",
    "        self.share = share\n",
    "        self.temperature = temperature\n",
    "        self.fixed = fixed\n",
    "\n",
    "        if share:\n",
    "            # weight: [input_dim, 1] or [input_dim, hidden_size] -> [hidden_size, 1]\n",
    "            final_dim = 1\n",
    "        else:\n",
    "            # weight: [input_dim, input_dim] or [input_dim, hidden_size] -> [hidden_size, input_dim]\n",
    "            final_dim = input_dim\n",
    "\n",
    "        first_groups = 1\n",
    "        last_groups = 1\n",
    "\n",
    "        if affine_layers == 1:\n",
    "            last_affine_input_dim = input_dim\n",
    "            # (x, 1) for global case and (x, h) for split case.\n",
    "            if num_head > 1 and split_input:\n",
    "                last_groups = num_head\n",
    "            self.relu_affine = False\n",
    "        elif affine_layers == 2:\n",
    "            last_affine_input_dim = hidden_size * num_head\n",
    "            if num_head > 1:\n",
    "                # (1, h) for global case and (h, h) for split case.\n",
    "                last_groups = num_head\n",
    "                if split_input:\n",
    "                    first_groups = num_head\n",
    "            # Add a relu-affine with affine_layers=2.\n",
    "            self.relu_affine = True\n",
    "            self.first_affine = TdnnAffine(input_dim, last_affine_input_dim, context=context, bias=bias, groups=first_groups)\n",
    "            self.relu = torch.nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"Expected 1 or 2 affine layers, but got {}.\",format(affine_layers))\n",
    "\n",
    "        self.last_affine = TdnnAffine(last_affine_input_dim, final_dim * num_head, context=context, bias=bias, groups=last_groups)\n",
    "        # Dim=2 means to apply softmax in different frames-index (batch is a 3-dim tensor in this case).\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        @inputs: a 3-dimensional tensor (a batch), including [samples-index, frames-dim-index, frames-index]\n",
    "        \"\"\"\n",
    "        assert len(inputs.shape) == 3\n",
    "        assert inputs.shape[1] == self.input_dim\n",
    "\n",
    "        if self.temperature:\n",
    "            batch_size = inputs.shape[0]\n",
    "            chunk_size = inputs.shape[2]\n",
    "\n",
    "        x = inputs\n",
    "        if self.relu_affine:\n",
    "            x = self.relu(self.first_affine(x))\n",
    "        if self.num_head > 1 and self.temperature:\n",
    "            if self.fixed:\n",
    "                t = self.t\n",
    "            else:\n",
    "                t = 1 + self.t**2\n",
    "            x = self.last_affine(x).reshape(batch_size, self.num_head, -1, chunk_size) / t\n",
    "            return self.softmax(x.reshape(batch_size, -1, chunk_size))\n",
    "        else:\n",
    "            return self.softmax(self.last_affine(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentiveStatisticsPooling(torch.nn.Module):\n",
    "    \"\"\" An attentive statistics pooling.\n",
    "    Reference: Okabe, Koji, Takafumi Koshinaka, and Koichi Shinoda. 2018. \"Attentive Statistics Pooling \n",
    "               for Deep Speaker Embedding.\" ArXiv Preprint ArXiv:1803.10963.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, affine_layers=2, hidden_size=64, context=[0], stddev=True, stddev_attention=True, eps=1.0e-10):\n",
    "        super(AttentiveStatisticsPooling, self).__init__()\n",
    "\n",
    "        self.stddev = stddev\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        if self.stddev :\n",
    "            self.output_dim = 2 * input_dim\n",
    "        else :\n",
    "            self.output_dim = input_dim\n",
    "\n",
    "        self.eps = eps\n",
    "        self.stddev_attention = stddev_attention\n",
    "\n",
    "        self.attention = AttentionAlphaComponent(input_dim, num_head=1, share=True, affine_layers=affine_layers, \n",
    "                                                 hidden_size=hidden_size, context=context)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        @inputs: a 3-dimensional tensor (a batch), including [samples-index, frames-dim-index, frames-index]\n",
    "        \"\"\"\n",
    "        assert len(inputs.shape) == 3\n",
    "        assert inputs.shape[1] == self.input_dim\n",
    "\n",
    "        alpha = self.attention(inputs)\n",
    "\n",
    "        # Weight avarage\n",
    "        mean = torch.sum(alpha * inputs, dim=2, keepdim=True)\n",
    "\n",
    "        if self.stddev :\n",
    "            if self.stddev_attention:\n",
    "                var = torch.sum(alpha * inputs**2, dim=2, keepdim=True) - mean**2\n",
    "                std = torch.sqrt(var.clamp(min=self.eps))\n",
    "            else:\n",
    "                var = torch.mean((inputs - mean)**2, dim=2, keepdim=True)\n",
    "                std = torch.sqrt(var.clamp(min=self.eps))\n",
    "            return torch.cat((mean, std), dim=1)\n",
    "        else :\n",
    "            return mean\n",
    "\n",
    "    def get_output_dim(self):\n",
    "        return self.output_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6],\n",
       "         [15]],\n",
       "\n",
       "        [[24],\n",
       "         [33]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.sum(dim = 2, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.sum(dim = 2, keepdim = True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keepdim的话就是保持原来的维度不变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = AttentionAlphaComponent(39, num_head=1, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.rand(42,39,125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = attention(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.sum(alpha * input_, dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\admin\\Desktop\\打分表.csv\",error_bad_lines=False, encoding = \"utf8\")\n",
    "\n",
    "df.index = list(range(1,15))\n",
    "\n",
    "d = df.drop(['组号'],axis = 1)\n",
    "\n",
    "x = pd.DataFrame(pd.Series(d.T.mean()))\n",
    "x.rename(columns = {0:\"平均分\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>平均分</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      平均分\n",
       "1    3.50\n",
       "2    1.00\n",
       "3    1.00\n",
       "4    1.00\n",
       "5    3.50\n",
       "6    1.00\n",
       "7    1.00\n",
       "8    1.00\n",
       "9    1.00\n",
       "10   1.00\n",
       "11   1.00\n",
       "12   1.00\n",
       "13   2.75\n",
       "14  20.75"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.rename(columns = {0:\"平均分\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
