{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* q和k算完以后，最后两个维度都是时间为维度\n",
    "* 矩阵相乘都只看最后两个维度，然后只要第一个矩阵的最后一个维度和第二个矩阵的第一个维度相等就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 135, 135])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(5,3,135,39)\n",
    "k = torch.randn(5,3,135,39)\n",
    "z = torch.matmul(q, k.transpose(-2, -1)) \n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 135, 135])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(z,dim = -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 135, 39])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.randn(5,3,135,39)\n",
    "torch.matmul(z,v).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 通过上面其实可以看出来，向量又变回原来的维度了\n",
    "* 下面是完整的Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multi-head的本质其实是把原来的dimention拆成了head*sub_dimention,然后拿的sub_dimension去做的处理\n",
    "* 所以原来传的dimension是head*sub_dimension的大小，这里sub_dimension就是词向量的维度\n",
    "* Multi_head attention里面用的是还是普通的attention,反正都是只处理的传进来的数据的最后两个维度而已"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    #这边matmul完的最后一个维度就是k的时间维度,batch,heads,q_t,k_t\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    print(\"scores.shape_1:\",scores.shape)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        print(\"mask.shape:\", mask.shape)  # 又多了一个1，为什么呢？因为muti-head attention也是4个维度的，所以要再unsequeeze一下\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)  # 所有的那些mask对应的权重就都是1e-9\n",
    "        print(\"scores_mask.shape\",scores.shape)  # 维度没变\n",
    "        print(\"scores_mask:\",scores)\n",
    "    #这边softmax肯定是在时间维度上softmax的，所以dim = -1应该就是时间维度,batch,heads,\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    print(\"scores.shape_2:\",scores.shape)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads # d_model should be multiples of heads, 这个其实就是内部拆成多个head\n",
    "        self.h = heads\n",
    "        #q,k,v都有自己对应的linear层\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear transformation and split into N heads\n",
    "        #把原来的d_model拆成了batch_size, seq_len, head, 每个head对应的维度\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        print(\"k.shape_1\",k.shape)\n",
    "\n",
    "        # transpose to get dimensions bs * N * seq_len * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        print(\"k.shape_2\",k.shape)\n",
    "\n",
    "        # calculate attention，这边其实就是拿的sub_dimension在做attentioni\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        # output: (batch_size, seq_len, d_model)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head = MultiHeadAttention(heads=4 , d_model = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = torch.randn(5, 135, 512), torch.randn(5, 135, 512), torch.randn(5, 135, 512)  # q,k,v 其实可以都是和input_x维度完全一样的\n",
    "# 当然，可以去变换最后一个维度不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(5,1,135) # encoder的 mask是 batch_size, 1, time_step\n",
    "\n",
    "mask[0][0][-1] = 0 \n",
    "\n",
    "mask[0][0][-2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.shape_1 torch.Size([5, 135, 4, 128])\n",
      "k.shape_2 torch.Size([5, 4, 135, 128])\n",
      "scores.shape_1: torch.Size([5, 4, 135, 135])\n",
      "mask.shape: torch.Size([5, 1, 1, 135])\n",
      "scores_mask.shape torch.Size([5, 4, 135, 135])\n",
      "scores_mask: tensor([[[[ 2.9316e-01, -7.4257e-01, -4.4617e-01,  ..., -8.1529e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-2.5498e-01,  4.0815e-01,  5.3102e-01,  ...,  3.2908e-02,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-5.1913e-02, -3.4576e-01, -1.3457e-01,  ...,  6.6825e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          ...,\n",
      "          [-1.7907e-01, -4.3929e-01, -1.6597e-02,  ...,  3.0488e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-2.4973e-01, -3.8612e-01, -3.6779e-01,  ...,  2.5824e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-6.1649e-01,  1.6495e-01,  5.4718e-01,  ..., -5.4148e-02,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-3.0802e-02, -2.2213e-01, -1.3141e-01,  ...,  5.5849e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-2.4604e-01,  6.8386e-01,  2.6209e-01,  ..., -4.7934e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0666e-01,  5.9272e-02,  4.3763e-01,  ..., -2.5296e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          ...,\n",
      "          [-7.3076e-02,  3.8562e-01, -2.7951e-01,  ...,  3.7674e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.5923e-01, -6.3318e-01,  3.5026e-01,  ...,  2.7755e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.3179e-01,  9.4255e-01,  2.4212e-01,  ..., -4.8135e-01,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[-4.0672e-01,  2.6880e-01,  3.9725e-01,  ...,  6.8698e-02,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [ 4.6035e-02, -4.1801e-01,  2.7944e-01,  ..., -7.0258e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-3.2926e-01, -2.6169e-02,  2.3922e-01,  ...,  5.0938e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          ...,\n",
      "          [-2.6048e-01,  2.4909e-01,  3.4223e-02,  ...,  7.6246e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [ 5.5646e-02,  4.5361e-01,  2.4339e-01,  ..., -6.3300e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [ 6.1378e-02,  4.2036e-01,  5.5102e-01,  ...,  4.5487e-01,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 3.3537e-01,  3.6939e-01, -4.1764e-01,  ...,  2.5164e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-2.5162e-01,  5.9521e-01,  4.5262e-01,  ..., -2.3134e-02,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [ 2.9240e-01, -3.6702e-01, -2.9886e-01,  ...,  2.4757e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          ...,\n",
      "          [-5.4607e-01, -3.3175e-01, -3.3060e-01,  ..., -1.4610e-02,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-2.4222e-02, -2.0593e-01, -2.7103e-03,  ...,  1.0040e-01,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-4.0094e-01, -6.9867e-02, -3.1498e-01,  ..., -3.9074e-01,\n",
      "           -1.0000e+09, -1.0000e+09]]],\n",
      "\n",
      "\n",
      "        [[[-1.1096e-01, -9.9266e-01,  6.0914e-02,  ..., -1.7956e-01,\n",
      "           -2.2446e-01, -8.6099e-01],\n",
      "          [ 3.1515e-01, -6.1719e-02,  3.5211e-01,  ...,  5.3313e-01,\n",
      "           -2.9355e-01, -7.6879e-02],\n",
      "          [-1.0716e-01, -1.4621e-01,  6.8344e-02,  ..., -8.8931e-01,\n",
      "            2.9231e-01,  5.3196e-01],\n",
      "          ...,\n",
      "          [ 2.0863e-01,  6.2706e-01,  1.9564e-01,  ...,  2.4794e-01,\n",
      "            6.1068e-02,  8.8353e-01],\n",
      "          [-5.3313e-01, -2.0061e-01, -2.1129e-01,  ..., -5.7491e-02,\n",
      "           -2.8726e-01,  5.6750e-01],\n",
      "          [-3.2881e-01,  4.0050e-01, -1.1512e-01,  ...,  3.2488e-01,\n",
      "            8.1286e-01,  6.1425e-01]],\n",
      "\n",
      "         [[ 8.3708e-02,  2.5142e-01,  4.9501e-01,  ...,  1.4176e-01,\n",
      "            1.8780e-01, -6.9533e-03],\n",
      "          [ 3.4333e-02,  1.2375e-01,  3.7328e-01,  ...,  4.1777e-01,\n",
      "            3.5290e-01,  1.4610e-01],\n",
      "          [ 5.5016e-01, -5.9266e-02, -3.4410e-01,  ..., -2.1846e-01,\n",
      "            3.0107e-01,  4.1002e-02],\n",
      "          ...,\n",
      "          [-3.3115e-01, -6.0254e-02,  2.9951e-01,  ...,  4.6524e-01,\n",
      "           -1.0939e-01,  1.6762e-01],\n",
      "          [ 5.5055e-01, -1.7025e-01,  6.4124e-01,  ...,  5.8473e-01,\n",
      "            7.9119e-01, -2.1798e-01],\n",
      "          [ 5.3899e-01, -2.0149e-02, -3.4286e-01,  ...,  5.0356e-01,\n",
      "           -5.7941e-01, -6.1204e-01]],\n",
      "\n",
      "         [[-1.5875e-01,  1.2647e-01,  4.2206e-01,  ...,  1.3010e-01,\n",
      "           -2.3822e-01,  6.2545e-02],\n",
      "          [-3.2395e-01,  7.9348e-01, -1.1513e-01,  ...,  4.6882e-02,\n",
      "           -1.7872e-01,  6.3389e-02],\n",
      "          [-1.7393e-01, -2.1307e-02,  4.2367e-02,  ..., -3.0133e-01,\n",
      "           -3.3993e-01, -2.1534e-01],\n",
      "          ...,\n",
      "          [ 6.2744e-02,  2.3067e-01,  3.4232e-02,  ...,  5.4511e-02,\n",
      "           -1.8391e-02, -3.7043e-01],\n",
      "          [ 3.0216e-02, -4.8038e-01,  1.9372e-02,  ...,  5.1965e-01,\n",
      "           -1.3349e-01, -1.8493e-01],\n",
      "          [-4.8205e-02, -3.9388e-01, -1.1090e-01,  ...,  2.5176e-01,\n",
      "           -1.3496e-01, -4.2998e-02]],\n",
      "\n",
      "         [[ 1.1441e-01, -1.4639e-02, -3.5637e-01,  ..., -2.5737e-01,\n",
      "            2.7954e-02,  1.0350e-01],\n",
      "          [-1.8762e-01, -1.5056e-01, -5.8765e-01,  ..., -3.6581e-01,\n",
      "            3.1534e-01,  1.4118e-02],\n",
      "          [ 5.3010e-02, -3.0520e-01, -4.1153e-01,  ..., -5.9904e-01,\n",
      "            6.3244e-01, -1.3684e-01],\n",
      "          ...,\n",
      "          [ 4.1997e-01,  3.0733e-01,  3.9197e-01,  ...,  2.8925e-01,\n",
      "           -1.0094e-01, -5.7691e-02],\n",
      "          [-2.7380e-01,  3.6488e-01, -2.7253e-01,  ..., -1.1041e-01,\n",
      "            4.6323e-01,  3.8021e-01],\n",
      "          [-2.5139e-01, -8.1568e-02, -4.3960e-01,  ..., -5.1014e-01,\n",
      "           -1.0843e-01,  1.1237e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8158e-02,  2.5451e-01,  2.2045e-02,  ...,  1.0921e-01,\n",
      "            5.8774e-02,  2.4932e-01],\n",
      "          [ 3.7969e-01,  1.3775e-01,  1.2163e-01,  ..., -2.3513e-02,\n",
      "           -1.2975e-01,  2.6339e-01],\n",
      "          [ 9.5937e-02,  1.8138e-01,  1.8146e-01,  ..., -1.0150e-01,\n",
      "           -2.6922e-01, -2.3166e-01],\n",
      "          ...,\n",
      "          [-6.9944e-01, -4.0765e-01,  4.0488e-01,  ...,  6.0307e-01,\n",
      "           -1.8621e-01, -1.8982e-01],\n",
      "          [ 4.6336e-01,  2.9035e-01, -2.7638e-01,  ..., -6.3496e-04,\n",
      "            1.8811e-01,  1.2246e-01],\n",
      "          [-8.4196e-04,  1.1316e-01,  1.8712e-01,  ...,  7.1571e-02,\n",
      "           -6.5280e-02, -4.3116e-01]],\n",
      "\n",
      "         [[-5.8812e-01,  2.0202e-01,  5.7983e-02,  ..., -4.1574e-01,\n",
      "            8.9376e-03,  4.6553e-01],\n",
      "          [-9.9060e-02, -4.8175e-01,  1.9206e-01,  ..., -5.0157e-01,\n",
      "           -1.5299e-02, -4.6044e-01],\n",
      "          [-9.6496e-02,  2.7633e-01, -3.1136e-01,  ...,  2.9364e-01,\n",
      "           -1.0147e-01,  5.4423e-01],\n",
      "          ...,\n",
      "          [ 2.9173e-02,  3.3856e-01,  4.4553e-02,  ...,  1.1901e-01,\n",
      "           -4.2934e-01, -2.9610e-02],\n",
      "          [ 2.7811e-01,  5.7951e-01, -2.2631e-01,  ..., -1.8617e-01,\n",
      "           -2.9431e-01, -5.1665e-02],\n",
      "          [ 1.5941e-02, -1.4912e-01,  2.7771e-01,  ..., -2.9226e-02,\n",
      "            1.2475e-01, -6.5566e-01]],\n",
      "\n",
      "         [[-2.7079e-01,  6.5838e-01, -2.5688e-01,  ..., -2.3666e-03,\n",
      "            9.8857e-02,  2.8628e-01],\n",
      "          [ 8.4897e-01, -3.1475e-01, -2.3870e-01,  ..., -8.8588e-02,\n",
      "           -5.1519e-01,  6.7963e-02],\n",
      "          [-1.9444e-02,  1.1442e-02,  2.5406e-01,  ..., -2.3665e-01,\n",
      "            1.9686e-01,  2.3607e-01],\n",
      "          ...,\n",
      "          [ 4.1676e-01,  1.3856e-01, -3.4050e-01,  ..., -1.4680e-01,\n",
      "           -1.8724e-01,  5.1692e-02],\n",
      "          [-2.7776e-01, -2.2793e-01,  4.3440e-02,  ..., -7.3008e-01,\n",
      "            2.2664e-01,  3.3341e-01],\n",
      "          [-3.0767e-01, -3.7650e-01,  5.7518e-01,  ...,  2.1472e-01,\n",
      "            1.3308e-01, -1.6600e-01]],\n",
      "\n",
      "         [[ 3.9566e-01, -1.7964e-01,  2.7826e-02,  ...,  1.5066e-01,\n",
      "           -3.4507e-01,  5.4813e-01],\n",
      "          [ 7.5341e-01, -3.4034e-01,  1.9963e-01,  ..., -2.4794e-01,\n",
      "           -9.1218e-02,  4.8028e-01],\n",
      "          [-4.1538e-01,  2.4257e-01, -3.2252e-01,  ..., -2.5723e-01,\n",
      "            1.3330e-02, -1.9138e-02],\n",
      "          ...,\n",
      "          [ 1.2567e-01,  2.4626e-01, -2.7172e-01,  ...,  2.5440e-01,\n",
      "            1.0745e-01, -2.3902e-01],\n",
      "          [-5.5134e-01,  2.3171e-01, -4.5696e-01,  ...,  3.2852e-01,\n",
      "            2.3124e-01,  4.4926e-01],\n",
      "          [-3.7050e-01,  4.7005e-02, -2.2073e-01,  ...,  3.6051e-01,\n",
      "            4.8544e-01,  5.2311e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6428e-01, -1.6787e-05,  3.5280e-01,  ..., -1.2036e-02,\n",
      "            4.0363e-01,  4.5755e-01],\n",
      "          [ 5.4761e-01,  5.3030e-01, -5.2069e-01,  ...,  4.1371e-01,\n",
      "            2.9507e-01,  1.4790e-01],\n",
      "          [ 8.8320e-02, -3.5434e-01, -2.7818e-02,  ...,  4.3625e-01,\n",
      "            1.0252e-01, -2.7884e-01],\n",
      "          ...,\n",
      "          [-3.9358e-01, -1.7117e-01,  2.1910e-01,  ..., -2.7415e-01,\n",
      "           -8.4433e-02, -2.2310e-01],\n",
      "          [ 2.6795e-01,  9.2172e-02, -1.7171e-01,  ..., -2.8678e-01,\n",
      "            2.9866e-01, -4.7384e-02],\n",
      "          [ 4.0791e-01, -3.0061e-01,  1.4096e-01,  ..., -1.1660e-01,\n",
      "            2.5392e-01,  3.4606e-02]],\n",
      "\n",
      "         [[ 2.3353e-02,  7.4068e-02,  2.3540e-01,  ...,  4.0792e-01,\n",
      "            5.1513e-02, -4.4714e-01],\n",
      "          [-2.1805e-01,  3.8783e-01,  4.4188e-02,  ...,  1.6359e-01,\n",
      "            1.3984e-01,  1.3210e-01],\n",
      "          [ 5.8290e-01,  2.3186e-01, -9.7910e-02,  ...,  8.0978e-01,\n",
      "            2.8934e-01,  7.4276e-02],\n",
      "          ...,\n",
      "          [-1.5696e-01,  5.6437e-01, -1.1590e-01,  ...,  2.5167e-01,\n",
      "            2.1343e-01,  2.8887e-02],\n",
      "          [ 2.0742e-02, -2.3043e-01,  6.5033e-02,  ..., -4.5366e-01,\n",
      "            2.9940e-02,  1.3703e-01],\n",
      "          [-4.3724e-01,  2.9074e-01,  1.3218e-01,  ...,  4.7843e-01,\n",
      "           -9.0230e-01,  2.1844e-02]],\n",
      "\n",
      "         [[-1.0961e-01,  1.1073e-01,  6.0179e-02,  ...,  6.5004e-01,\n",
      "           -9.7805e-02, -5.0766e-01],\n",
      "          [-1.0529e+00,  5.4362e-01,  6.4568e-02,  ..., -1.0481e+00,\n",
      "           -4.8474e-01, -2.4071e-01],\n",
      "          [ 1.3871e-01,  5.3575e-01, -1.2487e-01,  ..., -1.2515e-01,\n",
      "            2.8790e-01, -2.7402e-01],\n",
      "          ...,\n",
      "          [-6.3937e-01,  3.2742e-01,  5.9606e-02,  ...,  1.5201e-01,\n",
      "           -3.2389e-01, -1.3474e-01],\n",
      "          [-1.9640e-01, -1.5993e-01,  5.3620e-02,  ...,  1.6722e-01,\n",
      "           -1.3267e-01,  2.2564e-01],\n",
      "          [ 5.6157e-02,  1.4096e-01, -3.6133e-01,  ..., -3.6550e-01,\n",
      "           -8.1489e-02,  3.8175e-01]],\n",
      "\n",
      "         [[ 2.6226e-01,  2.0420e-01,  4.7821e-01,  ..., -1.2970e-01,\n",
      "            5.0665e-02, -7.1484e-02],\n",
      "          [-2.7824e-01,  5.1356e-01, -1.4611e-01,  ..., -1.0954e-02,\n",
      "           -4.4201e-01,  1.9361e-01],\n",
      "          [ 1.1693e+00,  4.9925e-01,  1.9135e-01,  ...,  1.1582e-01,\n",
      "           -1.8899e-01, -3.3728e-01],\n",
      "          ...,\n",
      "          [-6.9822e-02,  8.2016e-01,  1.3367e-01,  ..., -3.0198e-02,\n",
      "            2.0610e-02, -1.1889e-01],\n",
      "          [-7.2159e-02, -2.6560e-01, -1.4806e-01,  ..., -8.0227e-02,\n",
      "            3.2150e-01,  7.3440e-02],\n",
      "          [-7.1632e-01,  2.5505e-01,  5.5100e-01,  ..., -1.6761e-02,\n",
      "           -2.9199e-02,  2.4521e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4910e-01,  6.7987e-02,  3.0954e-01,  ..., -2.1705e-01,\n",
      "            1.1537e-02, -1.9550e-01],\n",
      "          [-4.9247e-01, -1.0209e+00,  3.1816e-01,  ..., -5.0657e-01,\n",
      "           -3.3906e-01, -6.7305e-01],\n",
      "          [ 9.1282e-02,  5.3780e-01, -5.4544e-04,  ...,  3.5312e-01,\n",
      "           -5.1625e-01, -4.0857e-01],\n",
      "          ...,\n",
      "          [-2.1785e-01,  7.5547e-01,  1.1140e-01,  ..., -1.4569e-01,\n",
      "            1.6362e-01, -6.1004e-01],\n",
      "          [ 3.1608e-01,  8.5223e-01, -3.1834e-01,  ...,  6.4438e-01,\n",
      "            1.6426e-01,  3.6951e-01],\n",
      "          [-2.6067e-01,  1.0214e+00,  8.1306e-02,  ...,  4.4186e-01,\n",
      "            4.9233e-02,  7.8870e-02]],\n",
      "\n",
      "         [[ 3.2116e-01,  2.7889e-01,  7.3976e-02,  ...,  2.1760e-01,\n",
      "           -1.9492e-01, -1.6228e-01],\n",
      "          [ 1.1679e-01,  9.0347e-02,  9.1468e-01,  ..., -5.3688e-01,\n",
      "           -1.4688e-01, -4.4138e-01],\n",
      "          [-1.9964e-01, -3.9879e-01, -7.1957e-02,  ...,  1.6819e-01,\n",
      "           -1.2395e-01, -2.8989e-01],\n",
      "          ...,\n",
      "          [-1.4536e-01, -4.0998e-01, -3.7658e-01,  ..., -5.2484e-01,\n",
      "            1.9548e-01,  3.4100e-01],\n",
      "          [ 2.3920e-01, -1.3747e-01, -3.5890e-01,  ..., -2.0331e-01,\n",
      "            2.4869e-01,  3.1702e-01],\n",
      "          [ 7.1061e-02,  4.8719e-01,  4.4795e-02,  ..., -2.9541e-01,\n",
      "            1.6068e-01,  3.9764e-02]],\n",
      "\n",
      "         [[-8.3169e-01,  2.6771e-01, -3.7893e-01,  ..., -3.4989e-01,\n",
      "           -4.8371e-01,  3.4274e-01],\n",
      "          [ 1.2585e-01,  2.6341e-01,  1.6754e-01,  ...,  5.7750e-02,\n",
      "           -5.8158e-02, -4.7005e-02],\n",
      "          [ 9.6762e-02, -1.1406e-01, -2.2695e-01,  ..., -1.8945e-01,\n",
      "           -7.8086e-02,  4.0718e-01],\n",
      "          ...,\n",
      "          [-7.4308e-02, -3.8338e-01, -4.0810e-01,  ..., -2.0950e-01,\n",
      "           -2.6493e-01, -1.2951e-01],\n",
      "          [ 4.8038e-02,  3.2800e-01,  4.2186e-02,  ..., -6.6868e-01,\n",
      "           -1.6214e-01,  9.1810e-03],\n",
      "          [ 2.2876e-01, -7.9382e-01,  6.4420e-01,  ...,  4.1663e-01,\n",
      "            1.7051e-01,  1.6558e-01]],\n",
      "\n",
      "         [[ 8.7570e-02, -4.0444e-01,  2.2361e-01,  ..., -1.6174e-01,\n",
      "            1.5030e-01,  1.2334e-02],\n",
      "          [ 6.4530e-02,  8.3376e-02, -1.5101e-01,  ...,  2.2833e-01,\n",
      "            6.5809e-02,  4.4570e-01],\n",
      "          [-3.1390e-01,  1.8991e-01, -1.3649e-01,  ...,  2.1146e-01,\n",
      "           -2.3818e-01,  2.8694e-01],\n",
      "          ...,\n",
      "          [ 5.8608e-01,  6.3224e-02, -6.6762e-02,  ...,  2.0964e-01,\n",
      "            2.0640e-03,  8.1816e-01],\n",
      "          [-1.4800e-02, -2.6470e-01, -1.9875e-01,  ..., -1.1275e-01,\n",
      "           -1.2728e-01, -2.4136e-01],\n",
      "          [-4.8134e-01, -1.9337e-01,  5.4709e-01,  ...,  5.6017e-01,\n",
      "            3.4285e-01, -4.0261e-01]]]], grad_fn=<MaskedFillBackward0>)\n",
      "scores.shape_2: torch.Size([5, 4, 135, 135])\n"
     ]
    }
   ],
   "source": [
    "out = multi_head(q, k, v,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 还是用的最后那两个维度去做q,k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 135, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总的来说就是，一开始传进来的是(batch_size, seq_len, d_model),d_model = head * word_embedding, 最后传出去的也是batch_size,seq_len,d_model\n",
    "### 和普通的attention唯一的不同就是\n",
    "* 普通的attention不是d_model, 直接就是word_embedding\n",
    "* Multi-head attention的d_model是head * word_embedding那么大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
