{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.function和tensor.function本质上是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果function名字后面多加了一个\"_\"就是修改自身的内容，否则就是返回一个副本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.Tensor和torch.tensor是不一样的，Tensor是empty和tensor的结合，empty在产生随机值的时候是需要同时给出dtype的\n",
    "#### Tensor是默认用FloatTensor dtype, tensor是推断最适合的dtype\n",
    "#### Tensor(1)是给出一个一维的随机值，tensor(1)就是给出1值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15.5397])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+000],\n",
       "       [7.66167459e-312]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.rand是均匀分布，torch.randn是标准正太分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.size()和torch.shape是一样的\n",
    "#### numel(tensor)是计算tensor里面元素的个数的 - number element\n",
    "#### view 和 reshape 作用相同，只是view出来的对象是原来的对象共享内存的，也就是里面的元素如果改变的话，原来的数据也是会改变的，reshape是返回副本，view(-1)是展平\n",
    "#### item是返回标量\n",
    "#### dim()是查看有几个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8969, -0.4493],\n",
       "        [-0.8164, -1.5378]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8969, -0.4493, -0.8164, -1.5378])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, -0.4493, -0.8164, -1.5378])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这里的x就已经改变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.4493],\n",
       "        [-0.8164, -1.5378]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask反正就是一个Bool型数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = x>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masked_select是把true的值选取出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(x,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.4493],\n",
       "        [-0.8164, -1.5378]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取非0元素的横纵坐标，下面这个就是四个元素的横纵坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这里大概的意思就是定义index来选取元素，index的shape就是最后输出的shape,index里面的内容，如果dim = 0就是按行取\n",
    "#### 也就是取第几行的元素，而index 元素所在的位置是第几列对应的元素的位置就是取第几列\n",
    "#### 比如0 1 0 就是取第一行第一个元素，第二行第二元素和第一行第三个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.LongTensor([[0,0]])\n",
    "a = torch.gather(x,0,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scatter和gather刚好相反，scatter是填充回去，gather是从里面拿出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.LongTensor([[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.4493],\n",
       "        [ 0.0000,  0.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.scatter_(0,index,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.4493]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐个元素的操作的英文名字叫做element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3)\n",
    "y = torch.randn(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 除法以后再乘再加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "addcdiv(input, tensor1, tensor2, *, value=1, out=None) -> Tensor\n",
       "\n",
       "Performs the element-wise division of :attr:`tensor1` by :attr:`tensor2`,\n",
       "multiply the result by the scalar :attr:`value` and add it to :attr:`input`.\n",
       "\n",
       ".. warning::\n",
       "    Integer division with addcdiv is no longer supported, and in a future release\n",
       "    addcdiv will perform a true division of :attr:`tensor1` and :attr:`tensor2`.\n",
       "    The historic addcdiv behavior can be implemented using :func:`floor_divide`\n",
       "    for integral inputs\n",
       "    (:attr:`input` + :attr:`value` * :attr:`tensor1` // :attr:`tensor2`)\n",
       "    and :func:`div` for float inputs\n",
       "    (:attr:`input` + :attr:`value` * :attr:`tensor1` / :attr:`tensor2`).\n",
       "    The future addcdiv behavior can be implemented with :func:`true_divide`\n",
       "    (:attr:`input` + :attr:`value` * torch.true_divide(:attr:`tensor1`,\n",
       "    :attr:`tensor2`).\n",
       "\n",
       ".. math::\n",
       "    \\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}\n",
       "\n",
       "\n",
       "The shapes of :attr:`input`, :attr:`tensor1`, and :attr:`tensor2` must be\n",
       ":ref:`broadcastable <broadcasting-semantics>`.\n",
       "\n",
       "For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be\n",
       "a real number, otherwise an integer.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the tensor to be added\n",
       "    tensor1 (Tensor): the numerator tensor\n",
       "    tensor2 (Tensor): the denominator tensor\n",
       "    value (Number, optional): multiplier for :math:`\\text{tensor1} / \\text{tensor2}`\n",
       "    out (Tensor, optional): the output tensor.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> t = torch.randn(1, 3)\n",
       "    >>> t1 = torch.randn(3, 1)\n",
       "    >>> t2 = torch.randn(1, 3)\n",
       "    >>> torch.addcdiv(t, t1, t2, value=0.1)\n",
       "    tensor([[-0.2312, -3.6496,  0.1312],\n",
       "            [-1.0428,  3.4292, -0.1030],\n",
       "            [-0.5369, -0.9829,  0.0430]])\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.addcdiv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9684,  6.0692,  0.5202],\n",
       "        [ 1.8168, -4.1906,  0.7733],\n",
       "        [-6.0507, 24.7901,  0.0584]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addcdiv(torch.rand(1,3),x,y,value = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4862,  0.3347,  0.3786],\n",
       "        [-0.5782, -0.1364,  1.2014]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dim等于多少就是把那个维度归并掉的意思,keepdim = True就是保留维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0460,  0.0992,  0.7900])\n",
      "tensor([[-0.0460,  0.0992,  0.7900]])\n"
     ]
    }
   ],
   "source": [
    "print(x.mean(dim = 0))\n",
    "print(x.mean(dim = 0, keepdim = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给出中位数，先排序才能给出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.median(\n",
       "values=tensor([-0.5782, -0.1364,  0.3786]),\n",
       "indices=tensor([1, 1, 0]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.median(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 累积，累加，维度不会变少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4862,  0.3347,  0.3786],\n",
       "        [-0.5782, -0.1364,  1.2014]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4862,  0.1627,  0.0616],\n",
       "        [-0.5782,  0.0789,  0.0947]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cumprod(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4862,  0.3347,  0.3786],\n",
       "        [-0.0920,  0.1983,  1.5799]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cumsum(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 所有元素的乘和加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6862)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0058)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eq比较两个tensor是否相等，支持broadcast\n",
    "#### equal是比较shape相不相等\n",
    "#### max,min(x, axis)如果有指定axis的话就会额外返回下角标\n",
    "#### topk(x,k,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4862,  0.3347,  0.3786],\n",
       "        [-0.5782, -0.1364,  1.2014]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4862, 0.3347, 1.2014]),\n",
       "indices=tensor([0, 0, 1]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.4862, 0.3347, 1.2014]),\n",
       "indices=tensor([0, 0, 1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 第一行是top1的值，第二行是top2的值，top几就会返回几行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[ 0.4862,  0.3347,  1.2014],\n",
       "        [-0.5782, -0.1364,  0.3786]]),\n",
       "indices=tensor([[0, 0, 1],\n",
       "        [1, 1, 0]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.topk(2,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch中的dot只能针对1-D使用，而numpy的dot则没有这个限制\n",
    "#### mm是对2-D使用的，bmm则是对3-D使用的\n",
    "#### View和转置都会导致数值在内存中存储不连续，所以用contiguous(邻近)来把他们弄成是连续的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### detach 和 with torch.no_grad()都是不再计算tensor的gradient\n",
    "#### grad_fn是非叶子结点是怎么通过叶子结点计算而来的那个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果是一个标量去调用backward()就不需要传参数了\n",
    "#### 如果需要梯度累积的话，backward(retain_graph = True)\n",
    "#### 如果是非标量而是tensor调用backward的话就得再传入gradient参数\n",
    "#### 雅可比矩阵就是矩阵形式的偏导数，矩阵里面的每个元素都是y和x元素排列组合组成的偏导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
