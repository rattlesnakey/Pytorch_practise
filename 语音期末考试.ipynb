{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. numpy转torch张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2,3,5)\n",
    "y = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. torch张量转numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. torch张量放到cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8605,  0.5746,  1.0204, -0.3501,  0.6064],\n",
       "         [-0.3705, -0.8260, -0.7749,  0.3132,  1.7654],\n",
       "         [-0.7184, -1.4276, -1.9096, -1.0018, -0.2816]],\n",
       "\n",
       "        [[-0.6227,  0.3883,  0.3244, -0.1308,  0.4275],\n",
       "         [-1.0647, -0.9039, -0.2666,  0.8682, -0.8307],\n",
       "         [ 1.2669,  1.3067, -1.2371,  0.1015,  0.5221]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. torch张量放到第4号GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* z.cuda(4)#这个是对的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. torch生成shape(B,C,T)的随机张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 42\n",
    "c = 39\n",
    "t = 135\n",
    "x = torch.rand(b,c,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. torch生成shape(B,C,T)的全零张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 135])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(b,c,t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. torch生成shape(B,C,T)的全壹张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 135])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(b,c,t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. torch生成输入39维输出128维的Linear层，并用随机数模拟输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(42, 39)\n",
    "Linear = nn.Linear(39,128)\n",
    "out = Linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. torch生成输入39通道输出128通道的CNN层，并用随机数模拟输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(in_channels = 39, out_channels = 128, kernel_size = 3)\n",
    "x = torch.rand(b,c,t)\n",
    "out = conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. torch生成输入39维输出128维的LSTM层，并用随机数模拟输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(t,b,c)#seq_len, batch_size, input_size\n",
    "lstm = nn.LSTM(input_size = 39, hidden_size = 128, num_layers = 2)\n",
    "out, hidden = lstm(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135, 42, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape#每一个时间步骤的最后一层隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 42, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape# 最后一个时间步骤的每一层的隐藏单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 42, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[1].shape# 最后一个时间步骤的每一层的隐藏单元所对应的memory cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 请指出1x1卷积和Linear层的相同与不同之处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)#batch,seq_len,feature_dim\n",
    "linear = nn.Linear(5,128)\n",
    "conv1 = nn.Conv1d(5,128,1)\n",
    "out1 = linear(x)\n",
    "out2 = conv1(x.transpose(1,2)).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 差别应该就是如果想要达到相同的效果的话,要把x 的feature_dim 转到通道上来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. LSTM层和CNN层的输入数据格式有什么区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTM的输入在没有设batch_first的时候是 seq_len, batch, input_dim(feature_dim)\n",
    "* CNN的输入是, batch, in_channels(input_dim,feature_dim), seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. 用torch代码实现对张量x进行sigmoid操作\n",
    "### 14. 用torch代码实现对张量x进行tanh操作\n",
    "### 15. 用torch代码实现对张量x进行relu操作\n",
    "### 16. 用torch代码实现对张量x进行softmax操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1547, 0.3182, 0.1835, 0.3436],\n",
       "         [0.2761, 0.2437, 0.2189, 0.2612],\n",
       "         [0.1767, 0.2928, 0.2389, 0.2916]],\n",
       "\n",
       "        [[0.3066, 0.1594, 0.3824, 0.1517],\n",
       "         [0.3110, 0.1921, 0.2171, 0.2799],\n",
       "         [0.3992, 0.2119, 0.2079, 0.1811]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "y = torch.tanh(x)\n",
    "y = torch.sigmoid(x)\n",
    "y = torch.relu(x)\n",
    "y = torch.softmax(x, dim = 2)# 记得要加参数dim,这边我设置的x的第二维是时间，在时间上softmax\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. 用torch代码实现对张量x进行概率0.5的dropout操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0963, 0.0000, 0.4374, 1.6917],\n",
       "         [0.0000, 1.0867, 0.0000, 1.2258],\n",
       "         [0.8975, 0.0000, 1.5005, 1.8991]],\n",
       "\n",
       "        [[0.0000, 0.0000, 1.9511, 0.1023],\n",
       "         [0.0000, 0.5788, 0.0000, 0.0000],\n",
       "         [1.6875, 0.0000, 0.3825, 0.0000]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = nn.Dropout(p = 0.5)\n",
    "y = drop(x)\n",
    "y#drop的点的值就是0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. 用torch代码实现张量的第2维和第1维交换位置\n",
    "### 19. 用torch代码实现张量x转成long类型\n",
    "### 20. 用torch代码实现张量x转成float类型\n",
    "### 21. 用torch代码实现形状(2,3,5)的张量转成(2,15)的张量\n",
    "### 22. 用torch代码实现将形状(2,3)的张量变为形状(2,3,1)的张量\n",
    "### 23. 用torch代码实现将形状(2,3,1)的张量变为形状(2,3)的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0482, 0.7690, 0.2187, 0.8458],\n",
       "         [0.6683, 0.5433, 0.4363, 0.6129],\n",
       "         [0.4487, 0.9539, 0.7503, 0.9496]],\n",
       "\n",
       "        [[0.7546, 0.1003, 0.9755, 0.0511],\n",
       "         [0.7710, 0.2894, 0.4114, 0.6656],\n",
       "         [0.8438, 0.2105, 0.1912, 0.0533]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.long()\n",
    "x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = x.view(2,-1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "y = x.unsqueeze(2)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. 用torch代码实现一个TDNN类，含有2个CNN层，扩张系数分别为2,4，其他参数自己定义，不得超过10行代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDNN(nn.Module):\n",
    "    def __init__(self, idim, odim, hdim=192): # 128+64\n",
    "        super(TDNN,self).__init__()\n",
    "        # help(torch.nn.Conv1d.__init__)\n",
    "        # torch.nn.Conv1d.__init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        # in_channels  输入通道数\n",
    "        # out_channels 输出通道数\n",
    "        # kernel_size  卷积核大小\n",
    "        # padding      两边补零数\n",
    "        # dilation     膨胀系数\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=idim, out_channels=hdim, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=hdim, out_channels=odim, kernel_size=3, padding=4, dilation=4)\n",
    "    def forward(self, x):\n",
    "        o1 = self.conv1(x).tanh()\n",
    "        o2 = self.conv2(o1)\n",
    "        o = o2.mean(dim = 2)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TDNN(39,128)\n",
    "x = torch.randn(42,39,135)\n",
    "y = model(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. 用torch代码实现一个Net类，含有1个CNN层和1个LSTM层，参数自己定义，要求注意forward时参数形状，不超过12行代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,feature_dim, hidden_dim, out_dim):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv = nn.Conv1d(feature_dim, hidden_dim, kernel_size = 1)\n",
    "        self.lstm = nn.LSTM(input_size = hidden_dim, hidden_size = out_dim, num_layers = 1)\n",
    "    def forward(self,x):\n",
    "        o1 = self.conv(x)# o1 - batch, hidden_dim, seq_len\n",
    "        o2 = o1.permute(2,0,1)\n",
    "        output, hidden = self.lstm(o2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_1(nn.Module):\n",
    "    def __init__(self,feature_dim, hidden_dim, out_dim):\n",
    "        super(Net_1,self).__init__()\n",
    "        self.gru = nn.GRU(input_size = feature_dim, hidden_size = hidden_dim, num_layers = 2)\n",
    "        self.conv = nn.Conv1d(hidden_dim, out_dim, kernel_size = 3)\n",
    "    def forward(self,x):\n",
    "        o1, hidden = self.gru(x)# o1 - time, batch, hidden_dim\n",
    "        o2 = o1.permute(1,2,0)#o2 - batch, feature_dim, time\n",
    "        output = self.conv(o2)\n",
    "        o = output.max(dim = 2).values#时间维度上平均池化\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 42])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net_1(39,128,42)\n",
    "x = torch.randn(135,5,39)#time, batch, feature_dim\n",
    "y = model(x)\n",
    "y.shape#(Batch,class_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_2(nn.Module):\n",
    "    def __init__(self,feature_dim, hidden_dim, out_dim):\n",
    "        super(Net_2,self).__init__()\n",
    "        self.conv = nn.Conv1d(feature_dim, hidden_dim, kernel_size = 3)\n",
    "        self.lstm = nn.LSTM(input_size = hidden_dim, hidden_size = out_dim, num_layers = 2)\n",
    "    def forward(self,x):\n",
    "        o1 = self.conv(x)# o1 - batch, hidden_dim, seq_len\n",
    "        o2 = o1.permute(2,0,1)\n",
    "        output, hidden = self.lstm(o2)#output是所有时间点最后一层隐藏层(time, batch, out_dim)\n",
    "        o = output.mean(dim = 0)#时间维度上平均池化\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 42])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net_2(39,128,42)\n",
    "x = torch.randn(5, 39, 135)#Batch, feature_dim, time\n",
    "y = model(x)\n",
    "y.shape#(Batch,class_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.rand(2,3,4)\n",
    "xx.max(dim = 2, keepdim = True).values.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. 用一行代码创建一个Adam优化器，优化的参数为net中的全部参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "initial_lr = 0.008\n",
    "optimizer_1 = torch.optim.Adam(model.parameters(), lr = initial_lr)\n",
    "scheduler_1 = LambdaLR(optimizer_1, lr_lambda=lambda epoch:1/(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. 用一行代码创建一个Adam优化器，优化的参数为net.cnn3中的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.conv.parameters(),lr = 0.0001)# 要大写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. 用torch代码创建交叉熵损失对象，并用正确形状的随机数模拟输入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 原来CrossEntropyLoss() 会把target变成ont-hot形式，我们现在例子的样本标签是[4]（从0开始计算）。那么转换成one-hot编码就是[0，0，0，0，1]，所以我们的最后也会变成一个5维的向量，并且不是该样本标签的数值为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0420)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 42)\n",
    "y_hat = torch.empty(5,dtype = torch.long).random_(42)#一个样本一个index - onehot, 需要是long数据类型.random_(42)是在0-41去选数字\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(y, y_hat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 0., 0., 4., 0., 2., 4., 1., 3., 1., 3.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.empty(11).random_(5)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. 用文字解释net.train()和net.eval()的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  net.train表明我的模型现在处于训练的状态，这时候网络层里面设置的dropout是开启的，这样可以有效防止过拟合，还有就是batchnorm, 训练的时候我们的数据是一个batch一个batch去处理的，这时候batchnorm的u和sigma是可以算出来的, 所以其**use_global_stats**参数在训练时设置为false;model.eval(),把模型设置为测试状态，这时候它的所有的参数都是固定的，不能再变化，还有就是dropout这时候也不执行，由于测试阶段不是对batch进行处理而是一条一条数据，所以batchnorm用的u和sigma是基于全局的已经算好的u和sigma, **use_global_stats**参数设置为True,这也是我之前看的那篇论文Adaptive batch norm 所改善的地方，让batch norm 的u和sigma在测试的时候能动态生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. 用文字解释optimizer.step()和optimizer.zero_grad()的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* optimizer.zero_grad()是对计算的梯度进行清0，如果不清零的话前面算的梯度会累积\n",
    "* optimizer.step()是用loss.backward()算好的梯度对optimizer里面所**管理的**参数进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. 用文字解释损失函数的backward()的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 算出来的loss值是一个scalar,loss.backward(),就是在**计算图**中基于当前loss值的**这个节点**,从后往前把所有节点的梯度**通过链式法则**都算出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2,2),requires_grad=True)\n",
    "y = x + 1\n",
    "z = (y*y+3).mean()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32. 指出下面代码错误的原因："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, data, label):\n",
    "    net.train()\n",
    "    predict = net(data)\n",
    "    net.optimizer.step()\n",
    "    net.optimizer.zero_grad()\n",
    "    net.loss(predict, label).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 要先zero_grad()梯度清零以后,再去反向传播算出计算图中各个节点的梯度,然后再step()，也就是基于算出来的梯度对参数进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33. 指出下面代码错误的原因并改正："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 5 at dimension 1, but got size 3 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fd558aa7f530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor to have size 5 at dimension 1, but got size 3 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,3,5)\n",
    "z = torch.matmul(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* matmul就是矩阵的相乘,要求两个数据要满足矩阵相乘的要求,也就是x的列维度要等于y的行维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,5,3)\n",
    "z = torch.matmul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 34. 指出下面代码错误的原因并改正："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b54eee2f838e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,3)\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"*\"的是进行数据的点乘，是把两个矩阵中相同对应位置的元素进行相乘，要求两个矩阵的维度个数要一样，且除维度值为1的(可以广播机制)其他维度的值要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,3,1)\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,3,5)\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,1,5)\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(1,1,5)\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35. 请文字说明DataSet中__getitem__的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __getitem__是用来决定dataset每次要返回的每条样本是什么，在执行dataset[i]的时候，调用的就是对象中__getitem__这个方法，一般都是返回feature,label这样的一条样本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 36. 请文字说明DataLoader中collate_fn的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collate_fn 就是一个自定义的对batch数据进行处理的函数，比如语音任务中一般就是在里面做一个Pad，然后把pad以后的数据返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37. 请文字说明DataLoader中shuffle的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* shuffle 就是对数据进行一个打乱，防止模型对越靠后的数据越敏感的现象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38. 请用torch随机生成形状(B,C,T)的张量，并在时间维度上进行全局最大池化\n",
    "### 39. 请用torch随机生成形状(B,C,T)的张量，并在时间维度上进行局部最大池化，核大小为3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(42,39,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.max(x,dim = 2, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1,2],[2,3],[3,4]],[[5,6],[7,8],[9,10]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 2,  3],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8],\n",
       "         [ 9, 10]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.max(x,dim = 2, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2],\n",
       "         [ 3],\n",
       "         [ 4]],\n",
       "\n",
       "        [[ 6],\n",
       "         [ 8],\n",
       "         [10]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.],\n",
       "         [ 3.],\n",
       "         [ 4.]],\n",
       "\n",
       "        [[ 6.],\n",
       "         [ 8.],\n",
       "         [10.]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.MaxPool1d(kernel_size = x.shape[2])\n",
    "z = model(x.float())\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 39, 133])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(42,39,135)\n",
    "model = nn.MaxPool1d(kernel_size = 3, stride = 1)#默认stride = kernel_size\n",
    "z = model(x.float())\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F.max_pool1d(x, kernel_size=x.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. 请用torch随机生成形状(B,C,T)的张量，并在通道维度上进行全局最大池化\n",
    "### 41. 请用torch随机生成形状(B,C,T)的张量，并在通道维度上进行局部最大池化，核大小为3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 1, 135])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.max(x, dim = 1, keepdim = True) # keepdim中间没有下划线\n",
    "y.values.shape#max以后记得要values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 37, 135])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = x.permute(0, 2, 1)\n",
    "z = model(x_1.float())\n",
    "u = z.permute(0,2,1)\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. 请用torch随机生成形状(2,3,5)和(2,3,7)两个张量，并拼接成(2,3,12)的张量\n",
    "* 在哪一维拼起来dim就等于几,记得前面是一个list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 12])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "y = torch.randn(2,3,7)\n",
    "z = torch.cat([x,y],dim = 2)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. 请用torch随机生成两个形状(3,5)的张量，并堆叠成(2,3,5)的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,5)\n",
    "y = torch.randn(3,5)\n",
    "z = torch.stack([x,y],dim = 0)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. 请文字描述torch.nn.Sequential的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 它是一个有序的容器，可以把多个首尾相连的层封装到里面来，注意，上一层数据的输出要符合下一层的输入，因为它是顺序传递的\n",
    "* 而且sequential内部的forward的功能已经实现了，不需要自己再实现\n",
    "* 当模型的前向计算为简单串联各个层的计算时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. 请用numpy将语音x的长度从15000补零到长度16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_frame = 16000\n",
    "data = [np.random.rand(i,39) for i in range(1,43)]#B,C',T\n",
    "pad_data = [np.pad(x,[(0,max_frame-len(x)),(0,0)]) for x in data]# [(第一个维度前面填充几位,第一个维度后面填充几位),(第二个维度...,..)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 16000, 39)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pad_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. 请用librosa读取wav文件语音并转成16000采样率单通道语音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "filename = librosa.example('nutcracker')\n",
    "y, sr = librosa.load(filename, sr = 16000, mono=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. 请用librosa去掉语音x中的静音\n",
    "* https://zhuanlan.zhihu.com/p/44189287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[  18944 1884160]\n"
     ]
    }
   ],
   "source": [
    "x, index = librosa.effects.trim(y,20)#20是top_db的阈值 \n",
    "#x是去除静音以后的序列,index是非静音段的开头索引和结束索引\n",
    "print(type(index))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. 请用librosa将语音x转为傅里叶谱spec，再逆变换回语音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = librosa.stft(y, n_fft=2048, window='hann')\n",
    "y_ = librosa.istft(spec, window='hann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1917952,)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_))\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. 请用soundfile将采样率fs的语音x保存到'out.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "fs = sr\n",
    "soundfile.write('out.wav', y, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. 请用提取mfcc特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3747)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "print(mfccs.shape)        # (40, 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11987, 13)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import python_speech_features\n",
    "wav_feature = python_speech_features.mfcc(y, sr)\n",
    "wav_feature.shape\n",
    "# '''\n",
    "# signal - 需要用来计算特征的音频信号，应该是一个N*1的数组\n",
    "# samplerate - 我们用来工作的信号的采样率\n",
    "# winlen - 分析窗口的长度，按秒计，默认0.025s(25ms)\n",
    "# winstep - 连续窗口之间的步长，按秒计，默认0.01s（10ms）\n",
    "# numcep - 倒频谱返回的数量，默认13\n",
    "# nfilt - 滤波器组的滤波器数量，默认26\n",
    "# nfft - FFT的大小，默认512\n",
    "# lowfreq - 梅尔滤波器的最低边缘，单位赫兹，默认为0\n",
    "# highfreq - 梅尔滤波器的最高边缘，单位赫兹，默认为采样率/2\n",
    "# preemph - 应用预加重过滤器和预加重过滤器的系数，0表示没有过滤器，默认0.97\n",
    "# ceplifter - 将升降器应用于最终的倒谱系数。 0没有升降机。默认值为22。\n",
    "# appendEnergy - 如果是true，则将第0个倒谱系数替换为总帧能量的对数。 \n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beat tracking example\n",
    "import librosa\n",
    "\n",
    "# 1. Get the file path to an included audio example\n",
    "filename = librosa.example('nutcracker')\n",
    "\n",
    "\n",
    "# 2. Load the audio as a waveform `y`\n",
    "#    Store the sampling rate as `sr`\n",
    "y, sr = librosa.load(filename)\n",
    "\n",
    "# 3. Run the default beat tracker\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "print('Estimated tempo: {:.2f} beats per minute'.format(tempo))\n",
    "\n",
    "# 4. Convert the frame indices of beat events into timestamps\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
