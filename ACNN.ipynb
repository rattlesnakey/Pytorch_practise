{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, in_channels, seq_len\n",
    "class ACNN(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, seq_len, N):\n",
    "        super(ACNN,self).__init__()\n",
    "        self.W_e = nn.Conv1d(in_channels, out_channels,kernel_size)\n",
    "        self.W_a = nn.Conv1d(in_channels, out_channels,kernel_size)\n",
    "        self.params = nn.ParameterDict({\n",
    "                'b_e': nn.Parameter(torch.randn(1)),\n",
    "                'b_a': nn.Parameter(torch.randn(1)),\n",
    "                'v':nn.Parameter(torch.rand(seq_len))\n",
    "        })\n",
    "        self.W_b_list = nn.ModuleList()\n",
    "        self.N = N\n",
    "        for i in range(N):\n",
    "            self.W_b_list.append(nn.Conv1d(in_channels = in_channels, \n",
    "                                        out_channels = out_channels, \n",
    "                                        kernel_size = kernel_size,bias = True))\n",
    "\n",
    "        # self.b_e = b_e\n",
    "        # self.b_a = b_a\n",
    "        # self.v = v #(seq_len,)\n",
    "        self.linear_combination = nn.Linear(seq_len*2,N)\n",
    "    def forward(self, x):\n",
    "        e_t = self.W_e(x) + self.params['b_e']#好像Bias = True就可以了\n",
    "        a_t = torch.matmul(torch.tanh(self.W_a(x) + self.params['b_a']),self.params['v'])\n",
    "        a_t = a_t.unsqueeze(2).expand(-1,-1,e_t.shape[2])\n",
    "        u = torch.sum(a_t * e_t, dim = 1)\n",
    "        sigma = torch.sqrt(torch.sum(a_t*(e_t*e_t),dim = 1) - u*u)#batch,seq_len\n",
    "        C_acnn = torch.cat([u,sigma], dim = 1)#batch,seq_len*2\n",
    "        beta = self.linear_combination(C_acnn)#batch,N 有N个filter\n",
    "        out = sum([conv(x)* beta.unsqueeze(1).unsqueeze(2).expand(-1,conv(x).shape[1],conv(x).shape[2]) for conv,beta in zip(self.W_b_list, beta.T)])/self.N\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acnn_layer = ACNN(512,512,1,135,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_layer = nn.Conv1d(512,512,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = torch.rand(10,512,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv1_layer(input_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 135])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 135])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acnn_layer(input_feature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchsnooper import snoop\n",
    "class ACNN(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, c, N):\n",
    "        super(ACNN,self).__init__()\n",
    "        self.W_e = nn.Conv1d(in_channels, in_channels,1, bias = True)\n",
    "        self.W_a = nn.Conv1d(in_channels, 1, 1, bias = True)\n",
    "        self.params = nn.ParameterDict({\n",
    "                'v':nn.Parameter(torch.rand(c))\n",
    "        })\n",
    "        self.W_b_list = nn.ModuleList()\n",
    "        self.N = N\n",
    "        for i in range(N):\n",
    "            self.W_b_list.append(nn.Conv1d(in_channels = in_channels, \n",
    "                                        out_channels = out_channels, \n",
    "                                        kernel_size = kernel_size,bias = True))\n",
    "        self.linear_combination = nn.Linear(in_channels*2,N)\n",
    "    @snoop()\n",
    "    def forward(self, x):\n",
    "        e_t = self.W_e(x)\n",
    "        a_t = torch.tanh(self.W_a(x))\n",
    "        a_t = a_t.expand(-1,e_t.shape[1],-1)\n",
    "        u = torch.sum(a_t * e_t, dim = 2)\n",
    "        sigma = torch.sqrt(torch.sum(a_t*(e_t*e_t),dim = 2) - u*u)\n",
    "        C_acnn = torch.cat([u,sigma], dim = 1)\n",
    "        beta = self.linear_combination(C_acnn)\n",
    "        out = sum([conv(x)* beta.unsqueeze(1).unsqueeze(2).expand(-1,conv(x).shape[1],conv(x).shape[2]) for conv,beta in zip(self.W_b_list, beta.T)])/self.N\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(42,39,135)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACNN(39,512,2,135,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source path:... <ipython-input-86-0564964b7bd3>\n",
      "Starting var:.. self = ACNN(  (W_e): Conv1d(39, 39, kernel_size=(1,), s...inear(in_features=78, out_features=3, bias=True))\n",
      "Starting var:.. x = tensor<(42, 39, 135), float32, cpu>\n",
      "15:44:39.921010 call        22     def forward(self, x):\n",
      "15:44:39.926994 line        23         e_t = self.W_e(x)\n",
      "New var:....... e_t = tensor<(42, 39, 135), float32, cpu, grad>\n",
      "15:44:39.930983 line        24         a_t = torch.tanh(self.W_a(x))\n",
      "New var:....... a_t = tensor<(42, 1, 135), float32, cpu, grad>\n",
      "15:44:39.935970 line        25         a_t = a_t.expand(-1,e_t.shape[1],-1)\n",
      "Modified var:.. a_t = tensor<(42, 39, 135), float32, cpu, grad>\n",
      "15:44:39.939959 line        26         u = torch.sum(a_t * e_t, dim = 2)\n",
      "New var:....... u = tensor<(42, 39), float32, cpu, grad>\n",
      "15:44:39.945943 line        27         sigma = torch.sqrt(torch.sum(a_t*(e_t*e_t),dim = 2) - u*u)\n",
      "New var:....... sigma = tensor<(42, 39), float32, cpu, grad, has_nan>\n",
      "15:44:39.951928 line        28         C_acnn = torch.cat([u,sigma], dim = 1)\n",
      "New var:....... C_acnn = tensor<(42, 78), float32, cpu, grad, has_nan>\n",
      "15:44:39.958908 line        29         beta = self.linear_combination(C_acnn)\n",
      "New var:....... beta = tensor<(42, 3), float32, cpu, grad, has_nan>\n",
      "15:44:39.964892 line        30         out = sum([conv(x)* beta.unsqueeze(1).unsqueeze(2).expand(-1,conv(x).shape[1],conv(x).shape[2]) for conv,beta in zip(self.W_b_list, beta.T)])/self.N\n",
      "New var:....... out = tensor<(42, 512, 134), float32, cpu, grad, has_nan>\n",
      "15:44:40.162365 line        31         return out \n",
      "15:44:40.192289 return      31         return out \n",
      "Return value:.. tensor<(42, 512, 134), float32, cpu, grad, has_nan>\n",
      "Elapsed time: 00:00:00.327125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.rand(42,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_b_list = nn.ModuleList()\n",
    "for i in range(3):\n",
    "    w_b_list.append(nn.Conv1d(in_channels = 39, \n",
    "                                out_channels = 512, \n",
    "                                kernel_size = 1,bias = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.rand(42,39,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = w_b_list[0](g).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sum([conv(g)* beta.unsqueeze(1).unsqueeze(2).expand(-1,conv(g).shape[1],conv(g).shape[2]) for conv,beta in zip(w_b_list, v.T)])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42])\n",
      "torch.Size([42, 1, 1])\n",
      "torch.Size([42, 512, 135])\n",
      "torch.Size([42])\n",
      "torch.Size([42, 1, 1])\n",
      "torch.Size([42, 512, 135])\n",
      "torch.Size([42])\n",
      "torch.Size([42, 1, 1])\n",
      "torch.Size([42, 512, 135])\n"
     ]
    }
   ],
   "source": [
    "for conv,beta in zip(w_b_list, v.T):\n",
    "    #转职一下就是把每一个conv的系数提到前面来，这样在for的时候就是一个一个取出来的\n",
    "    print(beta.shape)\n",
    "    print(beta.unsqueeze(1).unsqueeze(2).shape)\n",
    "    print(beta.unsqueeze(1).unsqueeze(2).expand(-1,conv(g).shape[1],conv(g).shape[2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = v.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 42])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 512, 135])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (135) must match the existing size (2) at non-singleton dimension 2.  Target sizes: [42, 512, 135].  Tensor sizes: [42, 1, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-ee5de3644748>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (135) must match the existing size (2) at non-singleton dimension 2.  Target sizes: [42, 512, 135].  Tensor sizes: [42, 1, 2]"
     ]
    }
   ],
   "source": [
    "k = j.expand(42,s[1],s[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0777, 0.3039, 0.2183, 0.6788, 0.5681, 0.6752, 0.2077, 0.6903, 0.5494,\n",
      "        0.9487, 0.3374, 0.5979, 0.4314, 0.2119, 0.8146, 0.8913, 0.5831, 0.3882,\n",
      "        0.2667, 0.2575, 0.1272, 0.0443, 0.4212, 0.8634, 0.8978, 0.8854, 0.5142,\n",
      "        0.6452, 0.7947, 0.5458, 0.7659, 0.0603, 0.1320, 0.4148, 0.8459, 0.0586,\n",
      "        0.6056, 0.6519, 0.9173, 0.5364, 0.8322, 0.1111])\n",
      "tensor([0.8051, 0.3497, 0.3009, 0.4961, 0.7294, 0.2902, 0.4880, 0.4592, 0.9567,\n",
      "        0.1518, 0.1337, 0.2897, 0.2017, 0.8400, 0.0687, 0.3372, 0.6599, 0.6890,\n",
      "        0.4962, 0.1497, 0.0524, 0.9495, 0.2119, 0.8436, 0.2531, 0.7784, 0.4149,\n",
      "        0.8176, 0.5467, 0.2176, 0.4249, 0.3640, 0.3473, 0.3341, 0.6063, 0.0459,\n",
      "        0.9975, 0.0631, 0.1189, 0.9486, 0.9079, 0.7226])\n"
     ]
    }
   ],
   "source": [
    "for conv, beta in zip(w_b_list, v.T):\n",
    "    print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
